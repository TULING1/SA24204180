---
title: "Homework"
author: "Zukun li"
date: "2024-12-08"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{homework}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# homework0


## 1美国七十个城市的年降水量分布

```{r}
data(precip)
hist(precip,
     main = "美国城市年降水量",
     xlab = "降水量",
     ylab = "频数"
     )

```

## 2美国年客运里程营收变化

```{r}
data(airmiles)
plot(airmiles)
```

对数据进行一元回归分析，即假设年客运营收$Y_{i}$和年数$X_{i}$符合 $$Y_{i}=\beta_{0}+\beta_{1}X_{i}$$ 由最小二乘法得到$${\hat{\theta_{1}}}=\frac{{\sum_{i}}(X_{i}-{\overline{X}})(Y_{i}-{\overline{Y}})}{{\sum_{i}}(X_{i}-{\overline{X}})^2} $$ $${\hat{\theta_{0}}}={\overline{Y}}-{\hat{\theta_{1}}}{\overline{X}} $$ 将回归方程作出

```{r}
df<-data.frame(Year = as.numeric(time(airmiles)),Miles = as.numeric(airmiles))
model <- lm(Miles~Year,data =df)
plot(df$Year, 
     df$Miles, 
     main="Air Miles Over Time",
     xlab="Year", 
     ylab="Miles",
     col="blue"
     )
abline(model, col="red")
```

## 3绘制指数分布图

```{r}
x<-rweibull(10000,shape=1,scale=0.06)
hist(x,
     xlab='频数',
     ylab='取值',
     density=10,
     nclass=100)
```

#homework1

## Question 3.4

The Rayleigh density [156, Ch. 18] is $$f(x)= \frac{x}{\sigma^{2}}e^{-\frac{x^{2}}{2\sigma^{2}}},x\ge0,\sigma^{2}>0$$ Develop an algorithm to generate random samples from a Rayleigh$(\sigma)$ distribution. Generate Rayleigh$(\sigma)$ samples for several choices of $\sigma>0$ and check that the mode of the generated samples is close to the theoretical mode

$\sigma$ (check the histogram).

## Answer 3.4

Let$t=x^{2}$.

So $f(t)=\frac{1}{2\sigma^{2}}e^{-\frac{t}{2\sigma^{2}}},F(t)=1-e^{-\frac{t}{2\sigma^{2}}}$.

Actually,$t\sim Exp(\frac{1}{2\sigma^{2}})$.

Then,let $u=F(t)=1-e^{-\frac{t}{2\sigma^{2}}}$.

So,$u\sim U(0,1)$.

In that way,$t=-2\sigma^{2}ln(1-u),x=\sqrt{t}$.Finally,we get: $$x=\sqrt{-2\sigma^{2}ln(1-u)}$$

Let$\sigma$ takes three different values:1,2,3

```{r}
set.seed(122)
n<- 1000
t1<-1 #σ's value
t2<-2
t3<-3
u<-runif(n,min=0,1-0.0001)
x1<-sqrt(-2*(t1**2)*log(1-u))
x2<-sqrt(-2*(t2**2)*log(1-u))
x3<-sqrt(-2*(t3**2)*log(1-u))
y<-seq(0,10,.01)
par(mfrow=c(1,3))
hist(x1,prob=TRUE,breaks=20,main ='σ=1')
lines(y,y/(t1**2)*exp(-y**2/(2*t1**2)))
hist(x2,prob=TRUE,breaks=20,main ='σ=2')
lines(y,y/(t2**2)*exp(-y**2/(2*t2**2)))
hist(x3,prob=TRUE,breaks=20,main ='σ=3')
lines(y,y/(t3**2)*exp(-y**2/(2*t3**2)))
```

From the result,we could find the simulation is good.

## Question 3.11

Generate a random sample of size 1000 from a normal location mixture. The

components of the mixture have $N(0,1)$ and $N(0,3)$ distributions with mixing

probabilities $p_{1}$ and $p_{2}=1-p_{1}$. Graph the histogram of the sample

with density superimposed, for $p_{1}=.75$. Repeat with different values for p1

and observe whether the empirical distribution of the mixture appears to be

bimodal. Make a conjecture about the values of p1 that produce bimodal

mixtures.

## Answer 3.11

The CDF of this distribution is:$$F(x)=p_{1}\Psi(x)+p_{2}\Psi(x),p_{1}+p_{2}=1$$

Let $p_{1}$ has the value:0.75,0.5,0.25

```{r}
set.seed(123)
p11=0.75
p21=0.5
p31=0.25
p12=1-p11
p22=1-p21
p32=1-p31
n<-1e4
x1<-rnorm(n,0,1)
x2<-rnorm(n,3,1)
r1<-sample(c(0,1),size = n,replace = TRUE,prob = c(p12,p11))
r2<-sample(c(0,1),size = n,replace = TRUE,prob = c(p22,p21))
r3<-sample(c(0,1),size = n,replace = TRUE,prob = c(p32,p31))
z1<-r1*x1+(1-r1)*x2
z2<-r2*x1+(1-r2)*x2
z3<-r3*x1+(1-r3)*x2
par(mfrow=c(1,3))
hist(z1,breaks = 50,main ="p1=0.75")
hist(z2,breaks = 50,main ="p1=0.5")
hist(z3,breaks = 50,main ="p1=0.25")
```

We found that the positions of these two peaks are located on 0 and 3,

respectively, which are the means of the original two distributions.What's

more,the probability of a function is higher,the peak of this function is

also higher.

## Question 3.20

A compound Poisson process is a stochastic process {$X(t),t≥ 0$}that can be

represented as the random sum $X(t)= \sum_{i=1}^{N(t)}Yi, t\ge0,$where{$N(t),t≥ 0$}

is a Poisson process and $Y1,Y2,...$are iid and independent of$N(t),t≥ 0$}.

Write a program to simulate a compound Poisson($\lambda$)–Gamma

process (Y has a Gamma distribution). Estimate the mean and the variance of

X(10) for several choices of the parameters and compare with the theoretical

values. Hint: Show that $E[X(t)] = λtE[Y_{1}]$ and $Var(X(t)) = λtE[Y_{1}^{2}]$.

## Answer 3.20

We know:$$P(N(t+s)-N(s)=n)=\frac{(\lambda t)^{n}}{n!}e^{-\lambda t}\ \ \ \ n=0,1,...$$ $$N(0)=0$$ And,$y_{i}$'s distribution is:$$ f(x)=\frac{\beta^{\alpha}x^{\alpha-1}}{\Gamma(x)}e^{-\beta x},x\ge0 $$ $$X(t)=\sum_{i=1}^{N(t)}Y_{i},t\ge0$$

```{r}
size<-1e4
t=10
lambda =3#the parameter of poisson process
shape<-1
scale<-1#the parameters of Gamma distribution
n<-qpois(1-1e-9,lambda =lambda*t)#the largest number which the poisson process can get
poip=function(z){
  
}
xn<-rpois(size,lambda*t)
xs<-c()
for(i in 1:size){
  yi=cumsum(c(rgamma(n,shape = shape,scale = scale)))
  xs<-c(xs,yi[xn[i]])
}
print("Mean and Variance of the sample")
print(mean(xs))
print(var(xs))
print("Theoretical mean and variance")
print(lambda*t*shape*scale)
print(lambda*t*(shape+1)*shape*scale**2)
```

We found that the mean and variance of the sample are very close to the

theoretical values

#homework2

## Question 5.4

Write a function to compute a Monte Carlo estimate of the Beta(3,3) cdf,and use the function to estimate $F(x)$ for $x=0.1,0.2,...,0.9$.Compare the estimates with the values returned by the $pbeta$ function in R.

## Answer

As $F(x)=\int_{-\infty}^{x}f(x)dx$ In this question,

$$\begin{align}
F(x) = &\int_{0}^{x}f(x;\alpha,\beta)dx\\
=& \int_{0}^{x}beta(x;3,3)dx\\
=&\int_{0}^{x}\frac{1}{B(3,3)}x^{3-1}(1-x)^{3-1}dxB\\
\end{align}
$$ We use Monte Carlo estimator: $$\begin{align}
&F(x) = E_{x}(\frac{1}{x\times b(3,3)}t^{2}(1-t)^{2})dt\\
&x\sim U(0,1)\\
&Its estimate:\hat{\theta}=\frac{1}{m}\sum_{i=1}^{m}\frac{1}{x\times b(3,3)}t_{i}^{2}(1-t_{i})^{2}
\end{align}
$$

```{r}
n=10000
Beta.cdf = function(x){
  y=runif(n)
  return(mean(dbeta(x*y,3,3))*x)
}
t<-seq(0.1,0.9,0.1)
x1<-sapply(t,function(t) pbeta(t,3,3))
x2<-sapply(t,function(t) Beta.cdf(t))
round(rbind(x1,x2),4)
```

## Question 5.9

The Rayleigh density is $$\\f(x)=\frac{x}{\sigma^{2}}e^{-x^{2}/(2\sigma^{2})},x\geq0,\sigma>0$$ Implement a function to generate samples from a Rayleigh($\sigma$)distribution, using antithetic variables.What is the percent reduction in variance of $\frac{X+X^{'}}{2}$compared with $\frac{X_{1}+X_{2}}{2}$for independent$X_{1},X_{2}$?

## Answer

Make $$\begin{align}
&u=1-e^{-\frac{x^{2}}{2\sigma^{2}}}\\
&u^{'}=1-u=e^{-\frac{x^{2}}{2\sigma^{2}}}\\
we\ get\  &u\sim U(0,1)\\
&x=\sigma\sqrt{-2ln(1-u)}\\
and\ &x^{'}=\sigma\sqrt{-2ln(u)}
\end{align}
$$ So we get the function below which can generate samples from a Rayleigh($\sigma$)distribution,using antithetic variables:

```{r}
n=10000
a = 2#value of σ
pdf1.Rayleigh = function(x){
  u1=runif(n/2)
  return(a*(-2*log(1-u1))**0.5)
}
pdf2.Rayleigh = function(x){
  u2=runif(n/2)
  return(a*(-2*log(u2))**0.5)
}
```

$$\begin{align}
as\ E(X)&=\int_{0}^{\infty}\frac{x^{2}}{\sigma^{2}}e^{-\frac{x^{2}}{2\sigma^{2}}}dx\\
&=\sqrt{\frac{\pi}{2}}\sigma\\
and\ Var(x)&=E(X^{2})-(E(X))^{2}\\
&=\int_{0}^{\infty}\frac{x^{3}}{\sigma^{2}}e^{-\frac{x^{2}}{2\sigma^{2}}}dx-\frac{\pi}{2}\sigma^{2}\\
&=(2-\frac{\pi}{2})\sigma^{2}\\
we\ can\ get\ \ Var(\frac{X_{1}+X_{2}}{2})&=(1-\frac{\pi}{4})\sigma^{2}\approx0.2146\sigma^{2}\\
and \ Var(\frac{X+X^{'}}{2})&=(1-\frac{\pi}{4})\sigma^{2}+\frac{Cov(X,X^{'})}{2}\\
because\ X&=\sigma\sqrt{-2ln(1-u)}\\
and\ X^{'}&=\sigma\sqrt{-2ln(u)}\ (u\sim U(0,1))\\
so\ Cov(X,X^{'})&=E(XX^{'})-E(X)E(X^{'})\\
&=\int_{0}^{1}2\sigma^{2}\sqrt{ln(x)ln(1-x)}dx-\frac{\pi}{2}\sigma^{2}\\
&\approx-0.4065\sigma^{2}\\
then\ Var(\frac{X+X^{'}}{2})&\approx0.0114\sigma^{2}\\
finally,we\ find\ the\ percent\\
reduction\ in\ varience\ is\ about:\\
&94.7\%
\end{align}
$$

## Question 5.13

Find two importance functions $f_{1}$and$f_{2}$that are supported on$(1,\infty)$and are 'closed' to $$g(x)=\frac{x^{2}}{\sqrt{2\pi}}e^{-x^{2}/2}\   \ x>1.
$$ Which of your two importance functions should produce the smaller variance in estimating $$\int_{1}^{\infty}\frac{x^{2}}{\sqrt{2\pi}}e^{-x^{2}/2}dx
$$ by importance sampling?Explain.

## Answer

```{r}
g = function (x) {
  x^2/sqrt(2*pi)*exp(-x^2/2)
}
x = seq(0,10,0.1)
y = g(x)
plot(x,y,type="l")

```

The probability distribution of the function is mainly concentrated in (1,4) Derive the function,we get:

$$
\frac{\partial g(x)}{\partial x}=\frac{1}{\sqrt{2\pi}}(2x-x^{3})e^{-\frac{x^{2}}{2}}=0\\
we\ find\ when\ x=\sqrt{2}\\
g(x)\ get\ the\ maximum\ value\sqrt{\frac{2}{\pi}}e^{-1}
$$ so,we choose the importance function $f_{1}\sim N(1.5,1)$and $f_{2}\sim Weibull(2,2)$

```{r}
g = function (x) {
  x^2/sqrt(2*pi)*exp(-x^2/2)
}
x = seq(1,10,0.1)
y = g(x)
plot(x,y,type="l",ylim=c(0,1))
y_norm<-dnorm(x,mean = 1.5, sd= 1)
y_weibull<-dweibull(x,shape = 2,scale = 2)
lines(x,y_norm,col="red",ylim=c(0,1))
lines(x,y_weibull,col="blue",ylim=c(0,1))
sd_norm<-sd(y/y_norm)
sd_weibull<-sd(y/y_weibull)
rbind(sd_norm,sd_weibull)
```

so ,the weibull distribution is better

## Question: Fast Sort

For $n=10^{4},2*10^{4},4*10^{4},6*10^{4},8*10^{4}$,apply the fast sorting algorithm to randomly permuted numbers of 1,...,n. Calculate computation time averaged over 100 simulations,denoted by$a_{n}$. Regress $a_{n}\ on\ t_{n}:=nlog(n)$,and graphically show the results(scatter plot and regression line).

## Answer

```{r chunk-name, eval=FALSE, echo=TRUE,results='asis'}
n=10000
t<-c(n,2*n,4*n,6*n,8*n)

##快速排序算法
quicksort=function(x){
    if(length(x)<=1)return(x)
    x0<-x[1]
    loc<-1
    low<-1
    n<-length(x)
    high<-n
    while(low!=high){
        if(loc==low){
            if(x[high]<x[loc]){
                tmp<-x[high]
                x[high]<-x[loc]
                x[loc]<-tmp
                low=low+1
                loc=high
            }else{
                    high=high-1
                }
            }else{
                if(x[low]>x[loc]){
                    tmp<-x[low]
                    x[low]<-x[loc]
                    x[loc]<-tmp
                    high=high-1
                    loc=low
                }else{
                    low=low+1
                }
           }
        }
    L=c()
    R=c()
    if(low>1) L=x[1:(low-1)]
    if(low<length(x)) R=x[(low+1):n]
    return(c(quicksort(L),x[low],quicksort(R)))
}

time<-numeric(5)
k<-0
for(i in t){
  k<-k+1
  random_sample=sample(1:n,i,replace=TRUE)
  x<-numeric(100)
  for(j in 1:100){
    start<-Sys.time()
    y<-quicksort(random_sample)
    end<-Sys.time()
    x[j]<-(end-start)
  }
  time[k]<-mean(x)
}
##回归分析
x_log_x<-t*log(t)
model<-lm(time ~ x_log_x)
y_pred<-predict(model)
polt_data<-data.frame(x_log_x=x_log_x,y=time,y_pred=y_pred)
ggplot(plot_data,aes(x=x_log_x))+
  geom_point(aes(y=y),color="blue")+  
  geom_line(aes(y=y_pred),color="red") + 
  labs(title="Regression of y on x * log(x)",
       x="x * log(x)",
       y="y")+
  theme_minimal()
```

#homework3


## Question 6.6

Estimate the 0.025,0.05,0.95,and 0.975 quantiles of the skewness$\sqrt{b_{1}}$ under normality by a Monte Carlo experiment.Compute the standard error of the estimates from(2.14)using the normal approximation for the density(with exact variance formula).Compare the estimated quantiles with the quantiles of the large sample approximation $\sqrt{b_{1}}\sim N(0,6/n)$.

## Answer

```{r}
n<-30
n_sim<-10000
skew_value<-numeric(n_sim)
for(i in 1:n_sim){
  sample_data<-rnorm(n)
  skew_value[i]<-sum((sample_data-mean(sample_data))^3)*(n^0.5)/((sd(sample_data)*(n-1)^0.5)^3)
}
quantiles<-quantile(skew_value,probs=c(0.025,0.05,0.95,0.975))
standard_error<-sqrt(6/n)
cat("estimate quantiles of skewness: \n")
print(quantiles)
cat("standard error:\n")
print(standard_error)
theory_quantiles<-qnorm(c(0.025,0.05,0.95,0.975),mean=0,sd=(6/n)^0.5)
cat("theoretical quantiles of large sample approximation:\n")
print(theory_quantiles)
```
we can get that the estimated value is very close to the theoretical value.

## Question 6.8

Refer to Example 6.16. Repeat the simulation, but also compute the $F$ test of equal variance, at significance level $\hat{a}\dot{=}0.055$.Compare the power of the Count Five test and $F$ test for small,medium,and large sample sizes.(Recall that the $F$ test is not applicable for non-normal distributions.)

## Answer

```{r}
count5test=function(x,y){
  X<-x-mean(x)
  Y<-y-mean(y)
  outx<-sum(X>max(Y))+sum(X<min(Y))
  outy<-sum(Y>max(X))+sum(Y<min(X))
  return(as.integer(max(c(outx,outy))>5))
}
sigma1 <- 1
sigma2 <- 1.5
alpha<-0.055
num<-c(5,20,100)
m<-1000
power_five<-power_F<-numeric(3)
count_five_power<-F_test_power<-numeric(3)
for(i in 1:3){
  count_five_reject<-0
  f_test_reject<-0
  for(j in 1:m){
    x<-rnorm(num[i], 0, sigma1)
    y<-rnorm(num[i], 0, sigma2)
    
    count_five_reject<-count_five_reject+count5test(x,y)
    f_test_reject<-f_test_reject+(var.test(x,y)$p.value<alpha)
  }
  count_five_power[i]<-count_five_reject/m
  F_test_power[i]<-f_test_reject/m
}
rbind(count_five_power,F_test_power)
```

We find ,as the sample size increases, the efficiency of both types of detection gradually increases.

## Question

If we obtain the powers for two methods under a particular simulation setting with 10,000 experiments: say, 0.651 for one method and 0.676 for another method. We want to know if the powers are different at 0.05 level.

1.What is the corresponding hypothesis test problem?

2.What test should we use? Z-test, two-sample t-test, paired-t test or McNemar test? Why?

3.Please provide the least necessary information for hypothesis testing.

## Answer

We can't say the powers are different at 0.05 level.

The corresponding hypothesis test problem is :if the powers of two methods are different,which is "$H_{0}: power_{1}=power_{2}$ vs $H_{1}: power_{1} \neq power_{2}$".

We should use paired-t test.Because there is only one sample for each of $power_{1}$ and $power_{2}$ in this question, so multiple experiments should be performed to generate a large of $power_{1}$ and $power_{2}$, thereby obtaining many differences of $power_{1}$ and $power_{2}$.

We can find that the difference between $power_{1}$ and $power_{2}$ is diff, a lot of diff approximately obey the t distribution. The null hypothesis can be tested by applying paired-t test.

#homework4


## Problem 1 

Of N = 1000 hypotheses, 950 are null and 50 are alternative.The p-value under any null hypothesis is uniformly distributed(use runif), and the p-value under any alternative hypothesis follows the beta distribution with parameter 0.1 and 1 (use rbeta). Obtain Bonferroni adjusted p-values and B-H adjusted p-values. Calculate FWER, FDR, and TPR under nominal level α =0.1 for each of the two adjustment methods based on m=10000 simulation replicates. You should output the 6 numbers (3 ) to a 3×2 table (column names: Bonferroni correction, B-H correction; row names: FWER, FDR, TPR). Comment the results.
```{r}
N<-1000
n_null<-950
n_alt<-50
alpha<-0.1
m<-1e4

results<-matrix(0, nrow = 3, ncol = 2)
rownames(results)<-c("FWER", "FDR", "TPR")
colnames(results)<-c("Bonferroni correction", "B-H correction")

for(i in 1:m){
  p_null<-runif(n_null)
  p_alt<-rbeta(n_alt,0.1,1)
  p_value<-c(p_null,p_alt)
  
  bonf_p<-p.adjust(p_value,method = "bonferroni")
  bh_p<-p.adjust(p_value,method="BH")
  
  reject_bonf<-bonf_p<alpha
  true_positive_bonf<-sum(reject_bonf[(n_null+1):N])
  false_positive_bonf<-sum(reject_bonf[1:n_null])
  fwer_bonf<-ifelse(false_positive_bonf>0,1,0)
  fdr_bonf<-ifelse(sum(reject_bonf)>0,false_positive_bonf/sum(reject_bonf),0)
  tpr_bonf<-true_positive_bonf/n_alt
  
  reject_bh<-bh_p<alpha
  true_positive_bh<-sum(reject_bh[(n_null+1):N])
  false_positive_bh<-sum(reject_bh[1:n_null])
  fwer_bh<-ifelse(false_positive_bh>0,1,0)
  fdr_bh<-ifelse(sum(reject_bh)>0,false_positive_bh/sum(reject_bh),0)
  tpr_bh<-true_positive_bh/n_alt
  
  results["FWER", "Bonferroni correction"]<-results["FWER", "Bonferroni correction"]+fwer_bonf
  results["FDR", "Bonferroni correction"]<-results["FDR", "Bonferroni correction"]+fdr_bonf
  results["TPR", "Bonferroni correction"]<-results["TPR", "Bonferroni correction"]+tpr_bonf
  
  results["FWER", "B-H correction"]<-results["FWER", "B-H correction"]+fwer_bh
  results["FDR", "B-H correction"]<-results["FDR", "B-H correction"]+fdr_bh
  results["TPR", "B-H correction"]<-results["TPR", "B-H correction"]+tpr_bh
}
result<-results/m
print(round(result,4))
```
B-H will give a larger value of TPR when its FDR is larger,it is suitable for more relaxed multiple comparison scenarios.

## 7.4
Refer to the air-conditioning data set aircondit provided in the boot package. The 12 observations are the times in hours between failures of air conditioning equipment:
3,5,7,18,43,85,91,98,100,130,230,487
Assume that the times between failures follow an exponential model Exp(λ). Obtain the MLE of the hazard rate λ and use bootstrap to estimate the bias and standard error of the estimate.

As the MLE of the hazard rate λ is $\frac{n}{\sum_{i=1}^{n}x_{i}}$
```{r}
library(boot)
set.seed(111)
hours<-c(3,5,7,18,43,85,91,98,100,130,230,487)
n<-12
B<-1000

mle=function(value,indices){
  return(length(value[indices])/sum(value[indices]))
}
lambda_hat<-mle(hours)
lambda_b<-boot(data = hours,statistic = mle,R = B)

round(c(bias=mean(lambda_b$t)-lambda_hat,standard_error=sd(lambda_b$t)),4)
```



## 7.5
Refer to Exercise 7.4. Compute 95% bootstrap confidence intervals for the mean time between failures 1/λ by the standard normal, basic, percentile, and BCa methods. Compare the intervals and explain why they may differ.

```{r}
set.seed(111)

mle2=function(value,indices){
  return(sum(value[indices])/length(value[indices]))
}
lambda_b_back<-boot(data = hours,statistic = mle2,R = B)
bootstrap_ci <- boot.ci(lambda_b_back,conf =0.95, type = c("norm","basic", "perc", "bca"))

print(bootstrap_ci)
```
The differences between each method stem from different assumptions about the null hypothesis and sample distribution, resulting in slightly different confidence intervals.

#homework5


# 7.8

Refer to Exercise 7.7. Obtain the jackknife estimates of bias and standard error of $\hat{\theta}$.

```{r}
library(bootstrap)
sigma<-cov(scor)
eigenvalue<-eigen(sigma)
value<-eigenvalue$values[order(-(eigenvalue$values))]
theta<-value[1]/sum(value)

n<-88
theta.hat<-numeric(n)
for(i in 1:n){
  jack.scor<-scor[-i,]
  sigma.hat<-cov(jack.scor)
  ss<-eigen(sigma.hat)
  vv<-ss$values[order(-(ss$values))]
  theta.hat[i]<-vv[1]/sum(vv)
}
jack.bias<-(n-1)*(mean(theta.hat)-theta)
jack.se<-sqrt((n-1)*mean((theta.hat-theta)**2))
print(round(c(jackknife.estimates.of.bias=jack.bias,jackknife.estimates.of.standerd.error=jack.se),4))
```

# 7.10

In Example 7.18, leave-one-out (n-fold) cross validation was used to select the best fitting model. Repeat the analysis replacing the Log-Log model with a cubic polynomial model. Which of the four models is selected by the cross validation procedure? Which model is selected according to maximum adjusted $R^{2}$?

```{r}
library(DAAG)
attach(ironslag)
n<-length(magnetic)   
e1<-e2<-e3<-e4<-numeric(n)
yhat1<-yhat2<-yhat3<-yhat4<-numeric(n)
SSR1<-SSR2<-SSR3<-SSR4<-SST1<-SST2<-SST3<-SST4<-numeric(n)
ybar<-mean(magnetic)
for (k in 1:n) {
  y<-magnetic[-k]
  x<-chemical[-k]

  J1<-lm(y ~ x)
  yhat1[k]<-J1$coef[1]+J1$coef[2]*chemical[k]
  e1[k]<-magnetic[k]-yhat1[k]
  SSR1[k]<-(yhat1[k]-ybar)^2
  SST1[k]<-(magnetic[k]-ybar)^2

  J2<-lm(y ~ x + I(x^2)) 
  yhat2[k]<-J2$coef[1]+J2$coef[2]*chemical[k]+
    J2$coef[3]*chemical[k]^2
  e2[k]<-magnetic[k]-yhat2[k]
  SSR2[k]<-(yhat2[k]-ybar)^2
  SST2[k]<-(magnetic[k]-ybar)^2

  J3<-lm(log(y) ~ x) 
  logyhat3<-J3$coef[1]+J3$coef[2]*chemical[k]
  yhat3[k]<-exp(logyhat3)
  e3[k]<-magnetic[k]-yhat3[k]
  SSR3[k]<-(yhat3[k]-ybar)^2
  SST3[k]<-(magnetic[k]-ybar)^2

  J4<-lm(y ~ x+I(x^2)+I(x^3)) 
  yhat4[k]<-J4$coef[1]+J4$coef[2]*chemical[k] +
  J4$coef[3]*chemical[k]^2+J4$coef[4]*chemical[k]^3
  e4[k]<-magnetic[k]-yhat4[k]
  SSR4[k]<-(yhat4[k]-ybar)^2
  SST4[k]<-(magnetic[k]-ybar)^2
}
print(c(mean(e1^2), mean(e2^2), mean(e3^2), mean(e4^2)))

print(c(51/50*sum(SSR1)/sum(SST1),51/49*sum(SSR2)/sum(SST2),51/50*sum(SSR3)/sum(SST3),51/48*sum(SSR4)/sum(SST4))) 
```

According to the prediction error criterion, Model 2, the quadratic model, would be the best fit for the data. What's more, the quadratic model has the largest $R^{2}$, so it is selected according to maximum adjusted $R^{2}$.

# 8.1

Implement the two-sample Cram´er-von Mises test for equal distributions as a permutation test. Apply the test to the data in Examples 8.1 and 8.2.

```{r eval = FALSE}

library(RVAideMemoire)
attach(chickwts)

x1=sort(weight[feed =="soybean"])
ys=sort(weight[feed == "linseed"])
xs=sample(x1, size = length(ys))
rep=1000

zs=c(xs, ys)
n1=length(xs)
n=n1*2

Ts=numeric(rep)
for (i in 1:rep) {
  ks=sample(1:n, n1, replace = FALSE)
  zs1=zs[ks]
  zs2=zs[-ks]
  Ts[i]=cramer.test(zs1, zs2)$statistic
}

(cvm=cramer.test(x, y))
T.hat=cvm$statistic
(p.hat=mean(abs(T.hat) < abs(Ts)))

hist(Ts)
```
 1 -dimensional  nonparametric Cramer-Test with kernel phiCramer 
(on equality of two distributions) 

	x-sample:  52  values        y-sample:  52  values

critical value for confidence level  95 % :  8.625 
observed statistic  3.75 , so that
	 hypothesis ("x is distributed as y") is  ACCEPTED .
estimated p-value =  0.2617383 

	[result based on  1000   ordinary  bootstrap-replicates]

0.99

# 8.2

Implement the bivariate Spearman rank correlation test for independence [255] as a permutation test. The Spearman rank correlation test statistic can be obtained from function cor with method = "spearman". Compare the achieved significance level of the permutation test with the p-value reported by cor.test on the same samples.

```{r}
soybean=chickwts$weight[chickwts$feed=="soybean"]
linseed =chickwts$weight[chickwts$feed=="linseed"]
n=length(soybean)
m=length(linseed)

tmp=min(n, m)
soybean=sort(soybean[1:tmp])
linseed=sort(linseed[1:tmp])

zs=c(soybean, linseed)
spearman.cor.test=cor.test(x = soybean, y = linseed, method = "spearman")

B=1000
k=length(zs)
rep<-1000
rhos=numeric(rep)

for(b in 1:B) {
  i=sample(1:k, k/2, replace = FALSE)
  xs=zs[i]
  ys=zs[-i]
  rhos[b]=cor(x = xs, y = ys, method = "spearman")
}

hist(rhos, breaks = 100)
(theta.hat=spearman.cor.test$estimate)
spearman.cor.test$p.value
(p.hat=mean(abs(rhos) > abs(theta.hat)))
(alpha=0.05)
```

We can find that p.hat\<alpha,so H0 rejected.


#homework6


## 9.3

Use the Metropolis-Hastings sampler to generate random variables from a standard Cauchy distribution. Discard the first 1000 of the chain, and compare the deciles of the generated observations with the deciles of the standard Cauchy distribution (see qcauchyor qt with df=1). Recall that a Cauchy(θ,η) distribution has density function

$$
f(x)=\frac{1}{\theta\pi(1+[(x-\eta)/\theta]^{2})}
$$

The standard Cauchy has the Cauchy(θ =1,η= 0) density. (Note that the standard Cauchy density is equal to the Student t density with one degree of freedom.)

```{r}
set.seed(111)
theta=1
eta=0


f<-function(x){
  stopifnot(theta > 0)
  return(1/(theta*pi*(1+((x-eta)/theta)**2)))
}

m<-1e4
k<-0
u<-runif(m)
x<-numeric(m)

x[1]<-rnorm(1,mean = 1)
for(i in 2:m){
  xt<-x[i-1]
  y<-rnorm(1,mean = xt)
  r=f(y)*dnorm(x=xt,mean=y)/(f(xt)*dnorm(x=y,mean = xt))
  if(u[i]<=r){
    x[i]=y
  }else{
    k=k+1
    x[i]=xt
  }
}
is = 5001:5500
par(mfrow = c(1,2))
plot(is, x[is], type="l")
a<-ppoints(100)
QR<-tan(pi*(a-0.5))
Q<-quantile(x, a)
qqplot(QR, Q, main="",
        xlab="Cauty Quantiles", ylab="Sample Quantiles")
abline(0,1,col='blue',lwd=2)
hist(x, probability = TRUE, breaks = 100)
plot.x = seq(min(x), max(x), 0.01)
lines(plot.x, f(plot.x))
par(mfrow = c(1,1))
```

## 9.8

This example appears in [40]. Consider the bivariate density

$$
f(x,y) \propto \binom{n}{x}y^{x+a-1}(1-y)^{n-x+b-1},\ x=0,1,...,n,0\le y\le 1
$$

It can be shown (see e.g. [23]) that for fixed a,b,n, the conditional distributions are Binomial(n,y)andBeta(x+a,n−x+b). Use the Gibbs sampler to generate a chain with target joint density f(x,y).

```{r}
n<-100
a<-20
b<-30
f=function(x,y){
  return(factorial(n)/(factorial(x)*factorial(n-x))*(y**(x+a-1))*((1-y)**(n-x+b+1)))
}
m<-5000
X=matrix(0,nrow = m,ncol = 2)
for(i in 2:m){
  xt=X[i-1,]
  xt[1]=rbinom(1,n,xt[2])
  xt[2]=rbeta(1,xt[1]+a,n-xt[1]+b)
  X[i,]=xt
}
x<-X[1001:m,]
cat('Means: ',round(colMeans(x),2))
cat('Standard errors: ',round(apply(x,2,sd),2))
cat('Correlation coefficients: ', round(cor(x[,1],x[,2]),2))

plot(x[,1],type='l',col=1,lwd=2,xlab='Index',ylab='Random numbers')
lines(x[,2],col=2,lwd=2)
legend('bottomright',c(expression(X[1]),expression(X[2])),col=1:2,lwd=2)
```

#homework7


## 11.3

(a)Write a function to compute the $k^{th}$ term in $$
\sum_{k=0}^{\infty}\frac{(-1)^{k}}{k!2^{k}}\frac{||a||^{2k+2}}{(2k+1)(2k+2)}\frac{\Gamma(\frac{d+1}{2})\Gamma(k+\frac{3}{2})}{\Gamma(k+\frac{d}{2}+1)}
$$ where d ≥ 1 is an integer, a is a vector in $R^{d}$,and \|\|.\|\|
denotes the Euclidean norm. Perform the arithmetic so that the
coefficients can be computed for (almost) arbitrarily large k and d.
(This sum converges for all $a\in R^{d}$. (b) Modify the function so
that it computes and returns the sum. (c) Evaluate the sum when
$a=(1,2)^{T}$.

```{r}
a<-c(1,2)
k_term = function(a,k){
  d<-length(a)
  return((-1)^k * exp((2*k+2)*log(norm(a,type="2"))-lgamma(k+1)-k*log(2)-log(2*k+1)-log(2*k+2)+lgamma((d+1)/2)+lgamma(k+3/2)-lgamma(k+d/2+1)))
}
n<-500
z<-numeric(n)
thesum = function(a){
  for (i in 1:n) {
    z[i]<-k_term(a,i-1)
  }
  return(sum(z))
}
thesum(a)
```

## 11.5

Write a function to solve the equation $$
\frac{2\Gamma(\frac{k}{2})}{\sqrt{\pi(k-1)}\Gamma(\frac{k-1}{2})}\int_{0}^{c_{k-1}}(1+\frac{u^{2}}{k-1})^{-k/2}du
\\=\frac{2\Gamma(\frac{k+1}{2})}{\sqrt{\pi k}\Gamma(\frac{k}{2})}\int_{0}^{c_{k}}(1+\frac{u^{2}}{k})^{-(k+1)/2}du\\
for\ a,where\\
c_{k}=\sqrt{\frac{a^{2}k}{k+1-a^{2}}}
$$ Compare the solutions with the points A(k) in Exercise 11.4.

```{r}
m <- c(5:25, 100, 500, 1000)
n <- c(4:25, 100)
A <- numeric(24)
B <- numeric(23)

for (k in m) {
f <- function(x){
       return(exp(log(2) + lgamma(x/2) - (1/2)*(log(pi*(x-1))) - lgamma((x-1)/2)))
}

g <- function(a){
       return((f(k)*integrate(function(x)(1 + (x^2)/(k-1))^(-k/2),lower = sqrt((a^2)*(k-1)/(k-a^2)), upper = Inf)$value) - (f(k+1)*integrate(function(x)(1 + (x^2)/k)^(-(k+1)/2), lower = sqrt((a^2)*k/(k+1-a^2)), upper = Inf)$value))
}
A[k] <- uniroot(g, lower = 0.01, upper = 1+sqrt(k)/2)$root
}

for (k in n) {
f <- function(x){
       return(exp(log(2) + lgamma(x/2) - (1/2)*(log(pi*(x-1))) - lgamma((x-1)/2)))
}
h <- function(a){
       return((f(k)*integrate(function(x)(1 + (x^2)/(k-1))^(-k/2), lower = 0, upper = sqrt((a^2)*(k-1)/(k-a^2)))$value) - (f(k+1)*integrate(function(x)(1 + (x^2)/k)^(-(k+1)/2), lower = 0, upper = sqrt((a^2)*k/(k+1-a^2)))$value))
}
B[k] <- uniroot(h, lower = 0.01, upper = 1+sqrt(k)/2)$root
}

A <- c(NA, A[m])
B <- c(B[n], NA, NA)
k <- c(4:25, 100, 500, 1000)
result <- data.frame(k, A, B)
knitr::kable(result)
```

We can find that when K is the same,A(k)=B(k)

Suppose T1,...,Tn are i.i.d. samples drawn from the exponential
distribution with expectation λ. Those values greater than τ are not
observed due to right censorship, so that the observed values are Yi =
TiI(Ti ≤ τ) + τI(Ti \> τ), i = 1,...,n. Suppose τ = 1 and the observed
Yi values are as follows: 0.54,
0.48,0.33,0.43,1.00,1.00,0.91,1.00,0.21,0.85 Use the E-M algorithm to
estimate λ, compare your result with the observed data MLE (note: Yi
follows a mixture distribution)

```{r}
Y <- c(0.54, 0.48, 0.33, 0.43, 1.00, 1.00, 0.91, 1.00, 0.21, 0.85) 
tau <- 1
censored <- Y == tau 
n <- length(Y)  

lambda <- 1 

tolerance <- 1e-6  
max_iter <- 1000  
for (iter in 1:max_iter) {
  Y_complete <- Y
  Y_complete[censored] <- tau + 1 / lambda
  lambda_new <- mean(1/Y_complete)
  
  if (abs(lambda - lambda_new) < tolerance) {
    cat("steps:", iter, "\n")
    break
  }

  lambda <- lambda_new
}


cat("lambda：", lambda, "\n")

lambda_mle_observed <- mean(1/Y)
cat("MLE：", lambda_mle_observed, "\n")

```


#homework8


## 11.7

Use the simplex algorithm to solve the following problem. Minimize 4x +2y +9z subject to $$
\begin{aligned}
2x+y+z&\le 2\\
x-y+3z&\le3\\
x\ge0,y\ge0,z&\ge0
\end{aligned}
$$

```{r,warning=FALSE}
library(lpSolve)

objective <- c(4, 2, 9)

constraints <- matrix(c(2, 1, 1,
                        1, -1, 3), 
                      nrow = 2, byrow = TRUE)

rhs <- c(2, 3)

direction <- c("<=", "<=")

solution <- lp("min", objective, constraints, direction, rhs, compute.sens = TRUE)

cat("Optimal value:", solution$objval, "\n")
cat("Optimal solution (x, y, z):", solution$solution, "\n")

```

# 11.1

## 3

Use both for loops and lapply() to fit linear models to the mtcars using the formulas stored in this list:

formulas \<- list( mpg \~ disp, mpg \~ I(1 / disp), mpg \~ disp + wt, mpg \~ I(1 / disp) + wt )

```{r}
formulas <- list(
 mpg ~ disp,
 mpg ~ I(1 / disp),
 mpg ~ disp + wt,
 mpg ~ I(1 / disp) + wt
 )

v1<-vector("list",4)
for (i in 1:4){
v1[i] <- lapply(i,function(i) {lm(formulas[[i]], data = mtcars)})
}
```

## 4

Fit the model mpg \~ disp to each of the bootstrap replicates of mtcars in the list below by using a for loop and lapply(). Can you do it without an anonymous function

bootstraps \<- lapply(1:10, function(i) { rows \<- sample(1:nrow(mtcars), rep = TRUE) mtcars[rows, ] })

```{r}
bootstraps <- lapply(1:10, function(i) {
 rows <- sample(1:nrow(mtcars), rep = TRUE)
 mtcars[rows, ]
})

v2<-vector("list",10)
for (i in 1:10){
v2[i] <- lapply(bootstraps[i], lm, formula = mpg ~ disp)
}
```

## 5

For each model in the previous two exercises, extract $R^{2}$ using the function below. rsq \<- function(mod) summary(mod)\$r.squared

```{r}
rsq <- function(mod) summary(mod)$r.squared
# question 3
sapply(v1, rsq)
#question 4
sapply(v2, rsq)
```

# 11.2

## 3

The following code simulates the performance of a t-test for non-normal data. Use sapply() and an anonymous function to extract the p-value from every trial.

trials \<- replicate( 100, t.test(rpois(10, 10), rpois(7, 10)), simplify = FALSE ) Extra challenge: get rid of the anonymous function by using [[ directly.

```{r}
trials <- replicate(
         100,
         t.test(rpois(10, 10), rpois(7, 10)),
         simplify = FALSE
       )
sapply(trials, function(x) x[["p.value"]])
sapply(trials, "[[", "p.value")
```

## 6

Implement a combination of Map() and vapply() to create a lapply() variant that iterates in parallel over all of its inputs and stores its outputs in a vector (or a matrix). What arguments should the function take?

# 17.5

## 4

Make a faster version of chisq.test() that only computes the chi-square test statistic when the input is two numeric vectors with no missing values. You can try simplifying chisq.test() or by coding from the mathematical definition.

```{r}
chisq_test_fast <- function(x, y) {
  if (!is.numeric(x) || !is.numeric(y)) {
    stop("Both inputs must be numeric vectors")
  }
  if (any(is.na(x)) || any(is.na(y))) {
    stop("Input vectors must not contain missing values")
  }
  n <- rbind(x, y)
  n1 <- rowSums(n)
  n2 <- colSums(n)
  k <- sum(n)
  m <- tcrossprod(n1,n2) / k

  x_stat <- sum((n - m)^2 / m)
  df <- (length(n1) - 1) * (length(n2) - 1)
  p.value <- pchisq(x_stat, df = df, lower.tail = FALSE)

  list(x_stat = x_stat, df = df, p.value = p.value)
}
```

## 5

Can you make a faster version of table() for the case of an input of two integer vectors with no missing values? Can you use it to speed up your chi-square test?

```{r}
library(fastmatch)
fast_table <- function(x, y) {
  if (!is.integer(x)) x <- as.integer(x)
  if (!is.integer(y)) y <- as.integer(y)
  
  a_sort <- sort(unique(a))
  b_ssort <- sort(unique(b))
  
  a_len <- length(a_sort)
  b_len <- length(b_sort)
  
  dim <- c(a_len, b_len)
  pr <- a_len * b_len
  dn <- list(a = a_sort, b = b_sort)
  
  bin <- fmatch(a, a_sort) + a_len * fmatch(b, b_sort) - a_len
  y <- tabulate(bin, pr)
  
  y <- array(y, dim = dim, dimnames = dn)
  class(y) <- "table"
  
  y
}
```

#homework9


This example appears in [40]. Consider the bivariate density
$$
f(x,y)\propto \left ( \begin{matrix}
n \\
x
\end{matrix}
\right )y^{x+a-1}(1-y)^{n-x+b-1},x=0,1,...,n,0\le y\le 1. 
$$
It can be shown (see e.g. [23]) that for fixed a,b,n, the conditional distributions are Binomial(n,y) and Beta(x+a,n−x+b). Use the Gibbs sampler to generate a chain with target joint density f(x,y).


```{r}
library(Rcpp)
library(SA24204180)
# 参数设置
n = 100
a = 10

# 运行R和Rcpp版本

x_r = gibbsR(n, a)
x_cpp = gibbsC(n, a)

qqplot(x_r[,1], x_cpp[,1], main = "QQ Plot for X", 
       xlab = "R Gibbs Samples", ylab = "Rcpp Gibbs Samples")
abline(0, 1, col = "red")

qqplot(x_r[,2], x_cpp[,2], main = "QQ Plot for Y", 
       xlab = "R Gibbs Samples", ylab = "Rcpp Gibbs Samples")
abline(0, 1, col = "red")

library(microbenchmark)

benchmark_results = microbenchmark(
  R_version = gibbsR(n, a),
  Rcpp_version = gibbsC(n, a),
  times = 10
)

print(benchmark_results)

```
